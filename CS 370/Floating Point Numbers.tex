%Notes by Harsh Mistry 
%CS 370
%Based on Template From  https://www.cs.cmu.edu/~ggordon/10725-F12/template.tex

\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}
\usepackage{amsmath,amsfonts,graphicx}
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   
   \graphicspath{ {images/} }
   
%Info Box 
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf CS 370 - Numerical Computation
	\hfill Fall 2018} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill  #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill Notes By: #4} }
      \vspace{2mm}}
   }
   \end{center}
   
   \markboth{Lecture #1: #2}{Lecture #1: #2}



 
}

\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}

\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{ex}[theorem]{Example}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}
\newcommand\E{\mathbb{E}}


%Start of Document 
\begin{document}

\lecture{1}{Floating Point Number Systems}{Christopher Batty}{Harsh Mistry}

\begin{definition}
\textbf{Infinite in extent} : There exists \(x\) such that \(verb|x|\) is arbitrarily large
\end{definition}

\begin{definition}
\textbf{Infinite in density} : Any interval \(a \leq x \leq b\) contains infinitely many numbers
\end{definition}

\begin{itemize}
\item Real $\mathbb{R}$ numbers are infinite in extent and infinite in density
\item Floating point systems are an approximate representation of real numbers using a finite number of bits
\item Floating point numbers don't behave like true real numbers due to rounding properties and truncation
\item We can express a real numbers an a infinite expansion relative to some base \(\beta\)
\item After expressing the real number in the desired base, we multiply by a power of \(\beta\) to shift it into a normalized form 
$$ 0.d_1d_2d_3d_4\ldots \times \beta^p$$ where 
\begin{itemize}
\item \(d_i\) are diguts in base \(\beta\)
\item normalized implies \(d_1 \neq 0\)
\item exponent \(p\) is an integer
\end{itemize}
\end{itemize}

\section{Floating Point}
\begin{itemize}
\item Density (or precision) is bounded by limiting the number of digits \(t\)
\item Extent (or range) is bounded by limited the range of values for exponent  \(p\)
\item The four integer parameters \(\{\beta, t, L, U\}\) characterize a specific floating point system \(F\)
\begin{itemize}
\item \(L\) and \(U\) bound \(p\) (The Exponent)
\item \(\beta\) is the base value
\item \(t\) is the number of digits the system can handle
\end{itemize}
\end{itemize}

\section{Overflow/underflow}
\begin{itemize}
\item if the exponent is too big or too small, our system cannot represent the number.
\item When this occurs, Overflow or Underflow will occur 
\end{itemize}

\section{Rounding vs. Truncation}
\begin{itemize}
\item \textbf{Rounding-to-nearest} - Rounds to closest available number in \(F\)
\item \textbf{Truncation} - Rounds to next number in F towards zero (i.e simply discard any digits after \(t^th\)
\end{itemize}

\section{Measuring Error}
\begin{itemize}
\item \(x_{exact}\) = true solution
\item \(x_{approx}\) = approximate solution 
\end{itemize}

using these values we can distinguish between absolute error and relative error 
\begin{itemize}
\item Absolute error 
$$ E_{abs} = |x_{exact} - x_{approx}|$$
\item Relative error 
$$ E_{ret} = \frac{|x_{exact} - x_{approx}|}{|x_{exact}|}$$
\end{itemize} 

\subsection{Relative Error and Significant Digits}
\begin{itemize}
\item Relative error is often more useful as its independent of the magnitudes of the number involved
\item Relative error also relates to the number of significant digits in the result. 
\end{itemize}

\subsection{Machine Epsilon}
\begin{itemize}
\item The maximum relative error, $E$, for a FP system is called machine epsilon or unit round of error.
\item It defined as the smallest value such that \(fl(1 + E) > 1\) under the given floating point system,
\item We can derive the rule
$$fl(x) = x (1 + \delta) \text{ for some } |\delta| \leq E $$1
\item With rounding to the nearest, $E = \frac{1}{2} \beta ^{1 -t}$
\item With Truncation, $E=\beta^{1-t}$
\item We care about the magnitude of E
\end{itemize}

\subsection{Arithmetic with Floating Point}
$$ w \oplus  z  = fl(w + z) = (w + z ) (1 + \delta) $$

\subsection{Catastrophic Cancellation Error}
Catastrophic cancellation occurs when subtracting numbers of about the same magnitude, and the input numbers contain error.
All significant digits cancelled out; the result might have no correct digits whatsoever!
However, if input quantities are known to be exact, we have our usual guarantee on floating point ops (addition, subtraction, etc.):

$$ w \ominus z  = (w - z) (1 + \delta) \text{ for } w, z, \in F$$

\section{Conditioning of Problems}
\begin{itemize}
\item Problems may be ill-conditioned or well-conditioned
\item For problem \(P\), with input \(I\) and output \(O\), of a change to the input, \(\Delta I\), gives a small change in the output \(\Delta O\), \(P\) is well-conditioned. Otherwise, \(P\) is ill-conditioned
\item This is a property of the problem it self and independent of any any specific implementation 
\end{itemize}

\subsection{Stability of an Algorithm}
If any initial error in the data is magnified by an algorithm, the algorithm is considered numerically unstable.

\subsection{Conditioning v.s. Stability }
\begin{itemize}
\item Conditioning of a problem is how sensitive the problem is itself to errors/changes in input
\item Stability of a numerical algorithm is how sensitive the algorithm is to errors/changes in input. 
\item An algorithm can be unstable even when the problem is well conditioned
\item An ill-conditioned problem limits how well we can expect any algorithm to perform. 
\end{itemize}

\section{Summary }
\begin{itemize}
\item \(F\) is not \(\mathbb{R}\)
\item We can use \textbf{round-off error analysis} to roughly bound the errors incurred by floating point operations.
\item We can analyse whether initial errors grow or shrink to determine the stability of algorithms.
\end{itemize}







\end{document}




