%Notes by Harsh Mistry 
%Stat231
%based on Template from : https://www.cs.cmu.edu/~ggordon/10725-F12/template.tex

\documentclass{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}
\usepackage{amsfonts,graphicx, amssymb}
\usepackage[fleqn]{amsmath}
\usepackage{fixltx2e}
\usepackage{tikz}
\usepackage{color}
\usepackage{tcolorbox}
\usepackage{lipsum}
\usepackage{listings}
\usepackage{scrextend}
\tcbuselibrary{skins,breakable}
\usetikzlibrary{shadings,shadows}
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   
   
%Info Box 
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf Stat 231 - Statistics 
	\hfill Spring 2017} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill Notes By: #4} }
      \vspace{2mm}}
   }
   \end{center}
   
   \markboth{Lecture #1: #2}{Lecture #1: #2}



 
}

\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
	
\newcommand{\pipe}{\(\mid\)}
\newcommand{\ctr}{\(\wedge\)}

\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{ex}[theorem]{Example}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}
\newcommand\E{\mathbb{E}}

%color definitions :
\definecolor{darkred}{rgb}{0.55, 0.0, 0.0}
\definecolor{lightcoral}{rgb}{0.94, 0.5, 0.5}
\definecolor{tomato}{rgb}{1.0, 0.39, 0.28}
\definecolor{lightgray}{rgb}{.9,.9,.9}
\definecolor{darkgray}{rgb}{.4,.4,.4}
\definecolor{purple}{rgb}{0.65, 0.12, 0.82}
\definecolor{lightgreen}{rgb}{0.56, 0.93, 0.56}
\definecolor{darkgreen}{rgb}{0.0, 0.2, 0.13}
\definecolor{limegreen}{rgb}{0.2, 0.8, 0.2}
\definecolor{lightblue}{rgb}{0.68, 0.85, 0.9}
\definecolor{darkblue}{rgb}{0.0, 0.0, 0.55}


%Environments
\newenvironment{exblock}[1]{%
    \tcolorbox[beamer,%
    noparskip,breakable,
    colback=lightgreen,colframe=darkgreen,%
    colbacklower=limegreen!75!lightgreen,%
    title=#1]}%
    {\endtcolorbox}

\newenvironment{ablock}[1]{%
    \tcolorbox[beamer,%
    noparskip,breakable,
    colback=lightcoral,colframe=darkred,%
    colbacklower=tomato!75!lightcoral,%
    title=#1]}%
    {\endtcolorbox}

\newenvironment{cblock}[1]{%
    \tcolorbox[beamer,%
    noparskip,breakable,
    colback=lightblue,colframe=darkblue,%
    colbacklower=darkblue!75!lightblue,%
    title=#1]}%
    {\endtcolorbox}


%Languages
\lstdefinelanguage{JavaScript}{
  keywords={typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, do, else, case, break},
  keywordstyle=\color{blue}\bfseries,
  ndkeywords={class, export, boolean, throw, implements, import, this},
  ndkeywordstyle=\color{darkgray}\bfseries,
  identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{purple}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  morestring=[b]',
  morestring=[b]"
}

%Listings
\lstset{
   language=JavaScript,
   backgroundcolor=\color{lightgray},
   extendedchars=true,
   basicstyle=\footnotesize\ttfamily,
   showstringspaces=false,
   showspaces=false,
   numbers=left,
   numberstyle=\footnotesize,
   numbersep=9pt,
   tabsize=2,
   breaklines=true,
   showtabs=false,
   captionpos=b
}


%Start
\begin{document}

\lecture{7, 8, 9, 10}{May 15th - 23rd, 2017}{Suryapratim Banerjee}{Harsh Mistry}

\section{Empirical C.D.F}
\textbf{Consider : } Data Set = \(\{y_1, \ldots, y_n\}\)

\begin{definition}
The empirical c.d.f F(y) is defined as 
$$ F(y) = \frac{\textbf{Number of observations } \leq y}{n} $$
The graph \((y, F(y))\) is the empirical C.D.F graph  
\end{definition}

\section{Box-Plot}
We use box plots to compare two or bore data sets to each other.
\begin{itemize}
\item Lower Side is \(Q_1\)
\item Upper Side is \(Q_3\)
\item Median is also marked: \(Q_2\)
\item Upper whisker stops at the maximum value of your data set which is less than or equal to \(Q_3  + 1.5 IQR\)
\item The lower whisker stops at the minimum value which is higher than or equal to \(Q_1 - 1.5 IQR\)
\item Any observations outside the whiskers are marked individually and are outliers
\end{itemize}

\section{Scatter Plots}
We use scatter plots to find whether there is an association between X and Y.\\
A scatter plot = \((x_1, y_1\) for \(i = 1, \ldots, n\)

\section{Types of Inference Problems }
An inference is when we use descriptive statistics to make statements about the population. The different types of inference problems are : 
\begin{itemize}
\item Estimation 
\item Testing of Hypothesis 
\item Prediction (Forecasting)
\end{itemize} 

\subsection{Estimation}

\begin{definition} The method by which we "guess" the population attributes using sample values. 
\end{definition}

\subsubsection*{Notations}
\begin{itemize}
\item Variables that are known are represented by Greek letters with a hat 
\item Variables that do not have a hat, represent unknown values 
\end{itemize}

\subsection{Hypothesis Testing Problems}
\begin{definition} A hypothesis is a claim made about the population 
\end{definition}


\subsection{Prediction Problems}
\begin{definition}
Prediction problems often consist of a data set and involve forecasting the future values 
$$ \hat{y}_{n+1} = ? \hspace{0.5cm} \hat{y}_{n+2} = ? $$
\end{definition}

\section{Statistical Models}
\textbf{Notation : }\(\theta\) represents  the population we are interested in. \\

A model in statistics is the \underline{Identification} of the distribution of the random variable \(Y_i\) from which \(y_i\) is drawn. Moreover \(\theta\) is a parameter of this distribution.
$$ Y \sim f(y ; \theta)$$

\begin{ex}
Trudeau's approval rating : \(\alpha\) - unknown \\

To determine the rating a sample of 100 voters and the number of voters who approve is found to be \(y = 120\).\\
In this case y is drawn from  \(Y \sim Bin(200, \alpha)\) 
\end{ex}

\section{The Theory of Estimation}
\(\theta\) is the population attribute that we are interested in, otherwise know as the \textbf{Parameter of Interest} and will never be known unless we have the entire population.\\

The objective is to fine an "estimate" of \(\theta\),using our sample observations \(\hat{\theta}\) .
$$ \hat{\theta} (y_1, \ldots, y_n) = \text{ Known \# once the sample is known} $$ 

\begin{enumerate}
\item Set up the statistical model. 
\item "Identify" the random variable from which your data set is drawn. 
\end{enumerate} 

\textbf{Note : } \(\theta\) is a parameter of the random variable

\section{Discrete Random Variables}
The likelihood function \(L(\theta ; y_1, \ldots, y_n)\) = Probability of observing your sample as a function of the unknown parameter \(\theta\) . \\

The maximum Likelihood Estimated (MLE) \(\hat{\theta}\) is the value maximizes \(L(\theta)\)
$$ L(\theta, y_1, \ldots, y_n) = P(Y_1 = y_1, Y_2, Y_2, \ldots, Y_n = y_n)$$

If the sample is independent and identically  (i.i.d.) 
$$ \begin{aligned}
L(\theta; y_1, \ldots, y_n) & = P(Y_1 = y_1, Y_2 = y_2, \ldots, Y_n = y_n) \\
& = P(Y_1 = y_i) \ldots P(Y_n = y_n) \\
& = f(y_1)f(y_2)\ldots f(y_n) \\
& = \prod_{i=1}^{n}f(y_i ; \theta) 
\end{aligned} $$

\subsection*{Geometric Model}
$$P(Y = y) = (1 - \theta)^y  \cdot \theta $$
$$ L(\theta) = (1 - \theta)^{y_1} \theta \ldots (1 - \theta)^{y_n} \theta = (1-\theta)^{\sum y_i} \theta^n $$
$$ l(\theta) = \sum y_i \cdot \ln(1 - \theta) + n \ln (\theta) $$

\subsection*{Binomial Model}
$$L(\lambda) = {}^nC_y \lambda^y (1-\lambda)^{n - y}  $$
$$l(\lambda) = \ln({}^nC_y) + y \ln (\lambda) + (n-y) \ln ( 1- pi) $$

\subsection*{Notes about the Likelihood Functions}
\begin{itemize}
\item One can think of the sample mean not just as a number, but also as an outcome of some r.v
\item \(L(\theta) = \prod_{i = 1}^n f(y_i ;\theta) \) : The value of the likelihood function is really small for large n. 
\end{itemize}

\subsection{Relative Likelihood Function}
$$R(\theta) = \frac{L(\theta)}{L(\hat{\theta})} , \text{ Where } \hat{\theta} = MLE $$ 
$$ R(\theta) \geq  0, \forall \theta $$
$$ R(\theta) = 1, \text{ if } \theta = \hat{\theta} $$
\textbf{Note : } The relative likelihood function also tells the "reasonable" values of \(\theta\) (The values that are "close" to \(\hat{\theta}\)
\end{document}
